# ğŸ“š Goodreads Book Scraper Project

---

## ğŸ” Overview

This project implements a **web scraper** to collect detailed metadata from the Goodreads **"Best Books Ever"** list. Using Python and powerful libraries like **Requests** and **BeautifulSoup**, the scraper extracts rich book info including:

- ğŸ“– Title & Author  
- ğŸ“ Description  
- ğŸ“š Genres  
- ğŸ“… Publication data  
- â­ Ratings  
- ğŸ“– Series details  

The output is a clean, structured dataset saved as a **CSV file**, ready for your data analysis and visualization adventures!

---

## ğŸš€ Current Functionality

- ğŸŒ Scrapes multiple Goodreads list pages to gather individual book URLs  
- ğŸ” Visits each bookâ€™s page and extracts comprehensive metadata  
- ğŸ”„ Handles network errors gracefully with retry logic for robustness  
- ğŸ§© Organizes data with Python **dataclasses** for clarity and easy transformation  
- ğŸ’¾ Saves aggregated data into a well-structured CSV (`goodreads-books-raw.csv`)  

---

## ğŸ”® Planned Enhancements

This project is just getting started! Upcoming phases include:

- ğŸ“Š **Exploratory Data Analysis (EDA):** Statistical summaries, distributions, and pattern discovery  
- ğŸ§¹ **Data Cleaning:** Handling missing values, correcting inconsistencies, and enriching data  
- ğŸ“ˆ **Data Analysis & Visualization:** Creating insightful charts and dashboards about books, authors, genres, and reader engagement  
- ğŸ¤– **Advanced Analytics:** Applying NLP on book descriptions and sentiment analysis on reviews  

---

## ğŸ› ï¸ Technologies Used

- Python 3.x  
- Requests  
- BeautifulSoup4  
- Pandas & NumPy  
- Dataclasses  
- Logging  
- Jupyter Notebook  

---

## âš™ï¸ Usage

1. Clone the repository  
2. Install dependencies from `requirements.txt`  
3. Run the Jupyter Notebook to execute the scraper and generate the dataset  
4. Use the generated CSV file for your analysis and visualization projects  

---

## âš ï¸ Notes

- Uses a custom **User-Agent** and polite scraping delays to avoid IP blocks  
- The dataset CSV is overwritten on each run to keep data fresh  

---


Thanks for checking out the project! Happy scraping and exploring! ğŸ“šâœ¨