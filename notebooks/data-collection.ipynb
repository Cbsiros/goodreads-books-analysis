{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c86de29",
   "metadata": {},
   "source": [
    "# üìö Goodreads Book Scraper Project\n",
    "\n",
    "This notebook uses web scraping to build a dataset from the [Goodreads \"Best Books Ever\"](https://www.goodreads.com/list/show/1.Best_Books_Ever) list. The final output is a clean dataset of popular books that can be used for analysis and visualization.\n",
    "\n",
    "## üîç What This Notebook Does\n",
    "\n",
    "- Collects links to individual book pages from Goodreads list pages\n",
    "- Extracts key metadata from each book page (title, author, description, etc.)\n",
    "- Cleans and organizes the data into a Pandas DataFrame\n",
    "- Saves the structured data to a CSV file for future use\n",
    "\n",
    "## üõ†Ô∏è Technologies Used\n",
    "\n",
    "- Python (general-purpose programming)\n",
    "- Requests (sending HTTP requests)\n",
    "- BeautifulSoup (HTML parsing)\n",
    "- Pandas & NumPy (for data handling)\n",
    "- Dataclasses (structured representation of book data)\n",
    "- Logging (runtime diagnostics and progress tracking)\n",
    "- Jupyter Notebook\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "- Uses a custom User-Agent to reduce the risk of being blocked\n",
    "- Implements retry logic and error handling for stability\n",
    "- Limits scraping scope per run to minimize load on Goodreads\n",
    "\n",
    "## üìÇ Output\n",
    "\n",
    "- `../data/goodreads-books-raw.csv` ‚Äî A structured file containing metadata for each book\n",
    "- The file is overwritten each time the scraper runs to maintain up-to-date results\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743c1f8",
   "metadata": {},
   "source": [
    "## Importing Libraries & Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fadef1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff13c1c",
   "metadata": {},
   "source": [
    "## Web Scraping Data from Goodreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8a7cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom user agent to avoid blocking by Goodreads\n",
    "user_agent = {'user-agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1859cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for Goodreads \"Best Books Ever\" list\n",
    "base_URL = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b045fd",
   "metadata": {},
   "source": [
    "### üîó Function: `collect_book_links`\n",
    "\n",
    "Retrieves book URLs from a single Goodreads list page. This function is the first step in the scraping pipeline, feeding individual book page links into the metadata extraction process.\n",
    "\n",
    "It constructs the correct URL based on a given page number, sends a request using a persistent `requests.Session`, and parses the HTML to find relative book links. These are then converted to full Goodreads URLs.\n",
    "\n",
    "The function is designed to handle transient network failures gracefully using built-in retry logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aced7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_book_links(\n",
    "    session: requests.Session,\n",
    "    page_num: int,\n",
    "    num_attempts: int = 3,\n",
    "    logging: bool = False\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Collects book page URLs from a specified Goodreads list page.\n",
    "\n",
    "    Parameters:\n",
    "        session (requests.Session): An active requests session used for sending HTTP requests.\n",
    "        page_num (int): The page number of the Goodreads list to scrape.\n",
    "        num_attempts (int, optional): Number of retry attempts in case of a failed request. Defaults to 3.\n",
    "        logging (bool, optional): If True, warning messages are logged for failed attempts. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of full URLs to individual Goodreads book pages.\n",
    "                   Returns an empty list if all attempts fail.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate URL for the specified page number\n",
    "    page_URL = base_URL + str(page_num)\n",
    "\n",
    "    for attempt in range(num_attempts):\n",
    "        try:\n",
    "            # Send HTTP GET request to the page\n",
    "            response = session.get(page_URL)\n",
    "\n",
    "            # Check if the response status code indicates failure\n",
    "            if response.status_code != 200:\n",
    "                if logging:\n",
    "                    logging.warning(f\"Failed to retrieve page {page_num}. Status code: {response.status_code}\")\n",
    "\n",
    "                # Sleep for a short, random time to avoid triggering any anti-scraping defenses\n",
    "                time.sleep(np.random.uniform(0.1, 1.0))\n",
    "                continue\n",
    "\n",
    "            # # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "             # Find all <a> tags with class 'bookTitle' - these contain links to individual book pages\n",
    "            book_links = soup.find_all('a', class_='bookTitle')\n",
    "\n",
    "            # Convert relative book URLs to absolute URLs\n",
    "            complete_book_links = [\"https://www.goodreads.com\" + link['href'] for link in book_links]\n",
    "\n",
    "            return complete_book_links\n",
    "\n",
    "        except Exception as e:\n",
    "            if logging:\n",
    "                    logging.warning(f\"Failed to retrieve page {page_num}. Status code: {response.status_code}\")\n",
    "            time.sleep(np.random.uniform(0.1, 1.0))\n",
    "            continue\n",
    "\n",
    "    # Return empty list if all attempts fail\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0068cef",
   "metadata": {},
   "source": [
    "### üß© Function: `safe_extract`\n",
    "\n",
    "This utility function provides a unified way to extract and clean HTML content from a parsed page. It abstracts away a lot of the repetitive logic that would otherwise clutter the metadata extraction process.\n",
    "\n",
    "It supports:\n",
    "\n",
    "- Extracting plain text or specific attributes from elements\n",
    "\n",
    "- Cleaning up inner HTML for complex fields like descriptions\n",
    "\n",
    "- Parsing embedded JSON (used to extract metadata like language)\n",
    "\n",
    "By centralizing error handling and content extraction, it keeps the main scraping functions concise and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ad3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_extract(\n",
    "    soup: BeautifulSoup,\n",
    "    selector: str,\n",
    "    attr: Optional[str] = None,\n",
    "    default: Optional[str] = None,\n",
    "    description: bool = False,\n",
    "    language: bool = False,\n",
    "    verbose: bool = False\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Safely extracts content from a BeautifulSoup object using a CSS selector.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML of the page.\n",
    "        selector (str): CSS selector string to identify the desired element.\n",
    "        attr (str, optional): The attribute to extract (e.g., \"href\", \"src\"). If None, extracts text. Defaults to None.\n",
    "        default (str, optional): Value to return if extraction fails. Defaults to None.\n",
    "        description (bool, optional): If True, cleans HTML inside descriptions. Defaults to False.\n",
    "        language (bool, optional): If True, parses JSON from a <script> tag to extract language info. Defaults to False.\n",
    "        verbose (bool, optional): If True, prints extraction errors to console. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: Extracted content as text or attribute value, or default if extraction fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        element = soup.select_one(selector)\n",
    "\n",
    "        if not element:\n",
    "            return default\n",
    "\n",
    "        elif description:\n",
    "            # Clean HTML tags and remove irrelevant inline elements\n",
    "            element_soup = BeautifulSoup(str(element), 'html.parser')\n",
    "            tags = element_soup.find_all('i')\n",
    "            for tag in tags:\n",
    "                if tag.find(\"a\") or \"ISBN\" in tag.text:\n",
    "                    tag.decompose()\n",
    "            return element_soup.get_text(separator=\" \").strip()\n",
    "\n",
    "        elif language:\n",
    "            # Extract language information from JSON data embedded in a <script> tag\n",
    "            json_data = json.loads(element.string)\n",
    "            return json_data.get(\"inLanguage\")\n",
    "\n",
    "        else:\n",
    "            # Return the requested attribute or the cleaned text content\n",
    "            return element[attr] if attr else element.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error in safe_extract for {selector}: {e}\")\n",
    "        return default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd69ff",
   "metadata": {},
   "source": [
    "### üì¶ Dataclass: `Book` \n",
    "Defines a consistent structure for storing the scraped book metadata. Using a @dataclass ensures type clarity, enforces field names, and simplifies downstream transformation into a DataFrame.\n",
    "\n",
    "The class includes fields for all expected metadata, including basic bibliographic info (title, author), content-related details (description, genres, language), statistics (ratings/reviews), and series information if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43130be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Book:\n",
    "    title: Optional[str]\n",
    "    author: Optional[str]\n",
    "    description: Optional[str]\n",
    "    genres: list[str]\n",
    "    language: Optional[str]\n",
    "    num_pages: Optional[str]\n",
    "    publication_year: Optional[str]\n",
    "    rating: Optional[str]\n",
    "    num_ratings: Optional[str]\n",
    "    num_reviews: Optional[str]\n",
    "    part_of_series: bool\n",
    "    series_name: Optional[str]\n",
    "    cover_link: Optional[str]\n",
    "    book_link: str\n",
    "    series_link: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce99f3d",
   "metadata": {},
   "source": [
    "### üìö Function: `collect_book_data`\n",
    "\n",
    "The main metadata extraction function. Given a Goodreads book URL, it fetches the HTML and parses all relevant fields.\n",
    "\n",
    "Handles:\n",
    "- Core content (title, author, genres)\n",
    "- User interaction stats (ratings, reviews)\n",
    "- Optional fields (description, series info)\n",
    "- JSON-parsed data (language metadata)\n",
    "\n",
    "The result is a complete `Book` dataclass object containing structured metadata for downstream use. Includes retry logic to handle failed requests gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a05e8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_book_data(\n",
    "    session: requests.Session,\n",
    "    book_link: str,\n",
    "    num_attempts: int = 3,\n",
    "    logging: bool = False\n",
    ") -> Optional[Book]:\n",
    "    \"\"\"\n",
    "    Extracts metadata from an individual Goodreads book page.\n",
    "\n",
    "    Parameters:\n",
    "        session (requests.Session): An active requests session used to send HTTP requests.\n",
    "        book_link (str): The full URL to a Goodreads book page.\n",
    "        num_attempts (int, optional): Number of retry attempts in case of request or parsing failure. Defaults to 3.\n",
    "        logging (bool, optional): If True, logs errors during failures. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Book]: A Book dataclass instance containing extracted metadata.\n",
    "                        Returns None if all attempts fail or required fields are missing.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(num_attempts):\n",
    "        try:\n",
    "            # Send HTTP GET request to the book page\n",
    "            response = session.get(book_link, headers=user_agent)\n",
    "\n",
    "            # Check for an unsuccessful response\n",
    "            if response.status_code != 200:\n",
    "                if logging:\n",
    "                    logging.warning(f\"Failed to retrieve book at link: {book_link}. Status code: {response.status_code}\")\n",
    "\n",
    "                time.sleep(np.random.uniform(0.1, 1.0))\n",
    "                continue\n",
    "\n",
    "            # Parse page content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract core book metadata\n",
    "            title = safe_extract(soup, \"h1.Text.Text__title1\")\n",
    "            author = safe_extract(soup, \"span.ContributorLink__name\")\n",
    "            description = safe_extract(soup, \"span.Formatted\", description = True)\n",
    "\n",
    "            # Extract genres\n",
    "            genres = [genre.text for genre in soup.select(\"span.BookPageMetadataSection__genreButton\")]\n",
    "\n",
    "            # Extract language info from JSON script tag\n",
    "            language = safe_extract(soup, \"script[type='application/ld+json']\", language = True)\n",
    "\n",
    "            # Extract number of pages and publication year\n",
    "            num_pages = safe_extract(soup, \"div.FeaturedDetails\")\n",
    "            num_pages = num_pages.split()[0] if num_pages is not None else None\n",
    "\n",
    "            publication_year = safe_extract(soup, \"div.FeaturedDetails\")\n",
    "            publication_year = publication_year.split()[-1] if publication_year is not None else None\n",
    "\n",
    "            # Extract user statistics\n",
    "            rating = safe_extract(soup, \"div.RatingStatistics__rating\")\n",
    "            num_ratings = safe_extract(soup, \"span[data-testid='ratingsCount']\")\n",
    "            num_ratings = num_ratings.replace(\",\", \"\").split(\"\\xa0\")[0] if num_ratings is not None else None\n",
    "            num_reviews = safe_extract(soup, \"span[data-testid='reviewsCount']\")\n",
    "            num_reviews = num_reviews.replace(\",\", \"\").split(\"\\xa0\")[0] if num_reviews is not None else None\n",
    "\n",
    "            # Extract cover image link and series data\n",
    "            cover_link = safe_extract(soup, \"img.ResponsiveImage\", attr=\"src\")\n",
    "            series_link = safe_extract(soup, \"h3.Text.Text__title3.Text__italic.Text__regular.Text__subdued a\", attr=\"href\")\n",
    "\n",
    "            part_of_series = series_link is not None\n",
    "            series_name = None\n",
    "\n",
    "            if part_of_series:\n",
    "                series_name = safe_extract(soup, \"h3.Text.Text__title3.Text__italic.Text__regular.Text__subdued a\")\n",
    "                series_name = series_name.split(\"#\")[0].strip() if series_name is not None else None\n",
    "\n",
    "            return Book(\n",
    "                title = title,\n",
    "                author = author,\n",
    "                description = description,\n",
    "                genres = genres,\n",
    "                language = language,\n",
    "                num_pages = num_pages,\n",
    "                publication_year = publication_year,\n",
    "                rating = rating,\n",
    "                num_ratings = num_ratings,\n",
    "                num_reviews = num_reviews,\n",
    "                part_of_series = part_of_series,\n",
    "                series_name = series_name,\n",
    "                cover_link = cover_link,\n",
    "                book_link = book_link,\n",
    "                series_link = series_link\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            if logging:\n",
    "                logging.error(f\"An error occurred on book found at {book_link}: {e}\")\n",
    "            time.sleep(np.random.uniform(0.1, 1.0))\n",
    "            continue\n",
    "\n",
    "    # Return None if all attempts fail\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fb4d1",
   "metadata": {},
   "source": [
    "### üöÄ Driver Code\n",
    "\n",
    "Coordinates the end-to-end scraping process.\n",
    "\n",
    "Steps:\n",
    "1. Sets up an HTTP session with a custom User-Agent\n",
    "2. Iterates through specified Goodreads list pages\n",
    "3. Collects links to each book on the page\n",
    "4. Extracts detailed metadata from each book\n",
    "5. Aggregates results into a list of `Book` objects\n",
    "6. Converts the list into a Pandas DataFrame\n",
    "7. Saves the final dataset to a CSV file\n",
    "\n",
    "Includes logging and delay mechanisms to monitor progress and minimize risk of IP blocks from Goodreads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c2b748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 16:56:54,036 [INFO] 10 books collected\n",
      "2025-07-01 16:57:13,132 [INFO] 20 books collected\n",
      "2025-07-01 16:57:26,154 [INFO] 30 books collected\n",
      "2025-07-01 16:57:38,251 [INFO] 40 books collected\n",
      "2025-07-01 16:57:52,496 [INFO] 50 books collected\n",
      "2025-07-01 16:58:05,955 [INFO] 60 books collected\n",
      "2025-07-01 16:58:17,467 [INFO] 70 books collected\n",
      "2025-07-01 16:58:30,093 [INFO] 80 books collected\n",
      "2025-07-01 16:58:41,542 [INFO] 90 books collected\n",
      "2025-07-01 16:58:53,433 [INFO] 100 books collected\n",
      "2025-07-01 16:58:53,487 [INFO] Scraping complete. 100 books collected\n",
      "2025-07-01 16:58:53,488 [INFO] Total scraping time: 2.26 minutes\n"
     ]
    }
   ],
   "source": [
    "data: list[Book] = []\n",
    "\n",
    "num_pages = 1        # Number of Goodreads list pages to scrape\n",
    "num_attempts = 3     # Retry attempts per request\n",
    "\n",
    "start = time.time()\n",
    "session = requests.Session()\n",
    "session.headers.update(user_agent)\n",
    "\n",
    "for page_num in range(num_pages): \n",
    "    book_links = collect_book_links(session, page_num + 1, num_attempts, logging = True)\n",
    "    \n",
    "    if not book_links:\n",
    "        logging.error(f\"No book links found on page {page_num + 1}. Ending collection.\")\n",
    "        break\n",
    "        \n",
    "    for book_link in book_links:\n",
    "        book_data = collect_book_data(session, book_link, num_attempts, logging = True)\n",
    "        if book_data is None or book_data.title is None:\n",
    "            continue\n",
    "        data.append(book_data)\n",
    "        if len(data) % 10 == 0:\n",
    "            logging.info(f\"{len(data)} books collected\")\n",
    "        time.sleep(0.05)  # Small delay between requests to reduce server load\n",
    "\n",
    "# Convert the list of Book dataclass instances to DataFrame \n",
    "books_df = pd.DataFrame([book.__dict__ for book in data])\n",
    "logging.info(f\"Scraping complete. {len(books_df)} books collected\")\n",
    "\n",
    "end = time.time()\n",
    "logging.info(f\"Total scraping time: {(end - start)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c14b17c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>description</th>\n",
       "      <th>genres</th>\n",
       "      <th>language</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>part_of_series</th>\n",
       "      <th>series_name</th>\n",
       "      <th>cover_link</th>\n",
       "      <th>book_link</th>\n",
       "      <th>series_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>Winning means fame and fortune. Losing means c...</td>\n",
       "      <td>[Young Adult, Dystopia, Fiction, Fantasy, Scie...</td>\n",
       "      <td>English</td>\n",
       "      <td>374</td>\n",
       "      <td>2008</td>\n",
       "      <td>4.35</td>\n",
       "      <td>9540867</td>\n",
       "      <td>249037</td>\n",
       "      <td>True</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-th...</td>\n",
       "      <td>https://www.goodreads.com/series/73758-the-hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>Since its immediate success in 1813,  Pride an...</td>\n",
       "      <td>[Classics, Romance, Fiction, Historical Fictio...</td>\n",
       "      <td>English</td>\n",
       "      <td>279</td>\n",
       "      <td>1813</td>\n",
       "      <td>4.29</td>\n",
       "      <td>4605711</td>\n",
       "      <td>135785</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1885.Pride...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>\"Shoot all the bluejays you want, if you can h...</td>\n",
       "      <td>[Classics, Fiction, Historical Fiction, School...</td>\n",
       "      <td>English</td>\n",
       "      <td>323</td>\n",
       "      <td>1960</td>\n",
       "      <td>4.26</td>\n",
       "      <td>6658748</td>\n",
       "      <td>127703</td>\n",
       "      <td>True</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2657.To_Ki...</td>\n",
       "      <td>https://www.goodreads.com/series/255474-to-kil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>It's official: the evil Lord Voldemort has ret...</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction, Magic, Audiobo...</td>\n",
       "      <td>English</td>\n",
       "      <td>896</td>\n",
       "      <td>2003</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3674570</td>\n",
       "      <td>73541</td>\n",
       "      <td>True</td>\n",
       "      <td>Harry Potter</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>https://www.goodreads.com/book/show/58613451-h...</td>\n",
       "      <td>https://www.goodreads.com/series/45175-harry-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Book Thief</td>\n",
       "      <td>Markus Zusak</td>\n",
       "      <td>It is 1939. Nazi Germany. The country is holdi...</td>\n",
       "      <td>[Historical Fiction, Fiction, Young Adult, Cla...</td>\n",
       "      <td>English</td>\n",
       "      <td>592</td>\n",
       "      <td>2005</td>\n",
       "      <td>4.39</td>\n",
       "      <td>2790109</td>\n",
       "      <td>155733</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>https://www.goodreads.com/book/show/19063.The_...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title           author  \\\n",
       "0                           The Hunger Games  Suzanne Collins   \n",
       "1                        Pride and Prejudice      Jane Austen   \n",
       "2                      To Kill a Mockingbird       Harper Lee   \n",
       "3  Harry Potter and the Order of the Phoenix     J.K. Rowling   \n",
       "4                             The Book Thief     Markus Zusak   \n",
       "\n",
       "                                         description  \\\n",
       "0  Winning means fame and fortune. Losing means c...   \n",
       "1  Since its immediate success in 1813,  Pride an...   \n",
       "2  \"Shoot all the bluejays you want, if you can h...   \n",
       "3  It's official: the evil Lord Voldemort has ret...   \n",
       "4  It is 1939. Nazi Germany. The country is holdi...   \n",
       "\n",
       "                                              genres language num_pages  \\\n",
       "0  [Young Adult, Dystopia, Fiction, Fantasy, Scie...  English       374   \n",
       "1  [Classics, Romance, Fiction, Historical Fictio...  English       279   \n",
       "2  [Classics, Fiction, Historical Fiction, School...  English       323   \n",
       "3  [Fantasy, Young Adult, Fiction, Magic, Audiobo...  English       896   \n",
       "4  [Historical Fiction, Fiction, Young Adult, Cla...  English       592   \n",
       "\n",
       "  publication_year rating num_ratings num_reviews  part_of_series  \\\n",
       "0             2008   4.35     9540867      249037            True   \n",
       "1             1813   4.29     4605711      135785           False   \n",
       "2             1960   4.26     6658748      127703            True   \n",
       "3             2003   4.50     3674570       73541            True   \n",
       "4             2005   4.39     2790109      155733           False   \n",
       "\n",
       "             series_name                                         cover_link  \\\n",
       "0       The Hunger Games  https://images-na.ssl-images-amazon.com/images...   \n",
       "1                   None  https://images-na.ssl-images-amazon.com/images...   \n",
       "2  To Kill a Mockingbird  https://images-na.ssl-images-amazon.com/images...   \n",
       "3           Harry Potter  https://images-na.ssl-images-amazon.com/images...   \n",
       "4                   None  https://images-na.ssl-images-amazon.com/images...   \n",
       "\n",
       "                                           book_link  \\\n",
       "0  https://www.goodreads.com/book/show/2767052-th...   \n",
       "1  https://www.goodreads.com/book/show/1885.Pride...   \n",
       "2  https://www.goodreads.com/book/show/2657.To_Ki...   \n",
       "3  https://www.goodreads.com/book/show/58613451-h...   \n",
       "4  https://www.goodreads.com/book/show/19063.The_...   \n",
       "\n",
       "                                         series_link  \n",
       "0  https://www.goodreads.com/series/73758-the-hun...  \n",
       "1                                               None  \n",
       "2  https://www.goodreads.com/series/255474-to-kil...  \n",
       "3  https://www.goodreads.com/series/45175-harry-p...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f99498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset to CSV\n",
    "books_df.to_csv('../data/goodreads-books-raw.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
