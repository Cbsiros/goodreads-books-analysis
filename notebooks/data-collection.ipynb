{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df5a182",
   "metadata": {},
   "source": [
    "# üìö Goodreads Book Data Web Scraper\n",
    "\n",
    "This project scrapes book metadata from the [Goodreads \"Best Books Ever\" list](https://www.goodreads.com/list/show/1.Best_Books_Ever) and compiles it into a clean, structured dataset. The output CSV is ready for downstream analysis, visualization, and data engineering workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê Web Scraping Workflow\n",
    "\n",
    "- Navigated Goodreads list pages to **collect links** to individual book detail pages.\n",
    "- Extracted core metadata from each book page\n",
    "- Structured the raw data into a **Pandas DataFrame** with consistent formatting and field alignment.\n",
    "- Saved the full dataset to a CSV file for persistent storage and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Engineering & Stability Features\n",
    "\n",
    "- Implemented **retry logic** and **exception handling** to gracefully handle failed requests or timeouts.\n",
    "- Used a **custom User-Agent** string to minimize detection and blocking by Goodreads.\n",
    "- **Rate-limited and scoped** requests to be respectful of Goodreads‚Äô servers.\n",
    "- Configured **logging output** to track progress and debug issues without interrupting the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Tools & Libraries\n",
    "\n",
    "- **Python**: Core scripting and data pipeline logic\n",
    "- **Requests**: Robust HTTP request handling\n",
    "- **BeautifulSoup**: HTML parsing and content extraction\n",
    "- **Pandas / NumPy**: Data wrangling and DataFrame construction\n",
    "- **Jupyter Notebook**: Interactive development and documentation\n",
    "\n",
    "---\n",
    "\n",
    "## üì§ Output\n",
    "\n",
    "- `../data/goodreads-books-raw.csv`: Dataset containing metadata for each scraped book.\n",
    "- This file is **overwritten on each run** to ensure fresh results without accumulating stale data.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Outcomes\n",
    "\n",
    "- Generated a production-style, reusable scraper that extracts real-world data for analysis.\n",
    "- Enabled future integration with tools like **Power BI**, **Tableu**, or **machine learning pipelines** by maintaining clean outputs.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743c1f8",
   "metadata": {},
   "source": [
    "## Importing Libraries & Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fadef1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Typing & structure\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Utilities & diagnostics\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Jupyter Notebook display settings\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "# Set up logging format and level\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff13c1c",
   "metadata": {},
   "source": [
    "## Web Scraping Data from Goodreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8a7cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom user agent to avoid blocking by Goodreads\n",
    "user_agent = {'user-agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1859cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for Goodreads \"Best Books Ever\" list\n",
    "base_URL = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b045fd",
   "metadata": {},
   "source": [
    "### üîó Function: `collect_book_links`\n",
    "\n",
    "Retrieves book URLs from a single Goodreads list page. This function is the first step in the scraping pipeline, feeding individual book page links into the metadata extraction process.\n",
    "\n",
    "It constructs the correct URL based on a given page number, sends a request using a persistent `requests.Session`, and parses the HTML to find relative book links. These are then converted to full Goodreads URLs.\n",
    "\n",
    "The function is designed to handle transient network failures gracefully using built-in retry logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aced7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_book_links(\n",
    "    session: requests.Session,\n",
    "    page_num: int,\n",
    "    num_attempts: int = 3,\n",
    "    log_warnings: bool = False\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Collects book page URLs from a specified Goodreads list page.\n",
    "\n",
    "    Parameters:\n",
    "        session (requests.Session): An active requests session used for sending HTTP requests.\n",
    "        page_num (int): The page number of the Goodreads list to scrape.\n",
    "        num_attempts (int, optional): Number of retry attempts in case of a failed request. Defaults to 3.\n",
    "        logging (bool, optional): If True, warning messages are logged for failed attempts. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of full URLs to individual Goodreads book pages.\n",
    "                   Returns an empty list if all attempts fail.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate URL for the specified page number\n",
    "    page_URL = base_URL + str(page_num)\n",
    "\n",
    "    for attempt in range(num_attempts):\n",
    "        try:\n",
    "            # Send HTTP GET request to the page\n",
    "            response = session.get(page_URL)\n",
    "\n",
    "            # Check if the response status code indicates failure\n",
    "            if response.status_code != 200:\n",
    "                if logging:\n",
    "                    logging.warning(f\"Failed to retrieve page {page_num}. Status code: {response.status_code}\")\n",
    "\n",
    "                # Sleep for a short, random time to avoid triggering any anti-scraping defenses\n",
    "                time.sleep(np.random.uniform(0.1, 1.0))\n",
    "                continue\n",
    "\n",
    "            # # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "             # Find all <a> tags with class 'bookTitle' - these contain links to individual book pages\n",
    "            book_links = soup.find_all('a', class_='bookTitle')\n",
    "\n",
    "            # Convert relative book URLs to absolute URLs\n",
    "            complete_book_links = [\"https://www.goodreads.com\" + link['href'] for link in book_links]\n",
    "\n",
    "            return complete_book_links\n",
    "\n",
    "        except Exception as e:\n",
    "            if logging:\n",
    "                    logging.warning(f\"Failed to retrieve page {page_num}. Status code: {response.status_code}\")\n",
    "            time.sleep(np.random.uniform(0.1, 1.0))\n",
    "            continue\n",
    "\n",
    "    # Return empty list if all attempts fail\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0068cef",
   "metadata": {},
   "source": [
    "### üß© Function: `safe_extract`\n",
    "\n",
    "This utility function provides a unified way to extract and clean HTML content from a parsed page. It abstracts away a lot of the repetitive logic that would otherwise clutter the metadata extraction process.\n",
    "\n",
    "It supports:\n",
    "\n",
    "- Extracting plain text or specific attributes from elements\n",
    "\n",
    "- Cleaning up inner HTML for complex fields like descriptions\n",
    "\n",
    "- Parsing embedded JSON (used to extract metadata like language)\n",
    "\n",
    "By centralizing error handling and content extraction, it keeps the main scraping functions concise and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ad3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_extract(\n",
    "    soup: BeautifulSoup,\n",
    "    selector: str,\n",
    "    attr: Optional[str] = None,\n",
    "    default: Optional[str] = None,\n",
    "    description: bool = False,\n",
    "    language: bool = False,\n",
    "    verbose: bool = False\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Safely extracts content from a BeautifulSoup object using a CSS selector.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML of the page.\n",
    "        selector (str): CSS selector string to identify the desired element.\n",
    "        attr (str, optional): The attribute to extract (e.g., \"href\", \"src\"). If None, extracts text. Defaults to None.\n",
    "        default (str, optional): Value to return if extraction fails. Defaults to None.\n",
    "        description (bool, optional): If True, cleans HTML inside descriptions. Defaults to False.\n",
    "        language (bool, optional): If True, parses JSON from a <script> tag to extract language info. Defaults to False.\n",
    "        verbose (bool, optional): If True, prints extraction errors to console. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: Extracted content as text or attribute value, or default if extraction fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        element = soup.select_one(selector)\n",
    "\n",
    "        if not element:\n",
    "            return default\n",
    "\n",
    "        elif description:\n",
    "            # Clean HTML tags and remove irrelevant inline elements\n",
    "            element_soup = BeautifulSoup(str(element), 'html.parser')\n",
    "            tags = element_soup.find_all('i')\n",
    "            for tag in tags:\n",
    "                if tag.find(\"a\") or \"ISBN\" in tag.text:\n",
    "                    tag.decompose()\n",
    "            return element_soup.get_text(separator=\" \").strip()\n",
    "\n",
    "        elif language:\n",
    "            # Extract language information from JSON data embedded in a <script> tag\n",
    "            json_data = json.loads(element.string)\n",
    "            return json_data.get(\"inLanguage\")\n",
    "\n",
    "        else:\n",
    "            # Return the requested attribute or the cleaned text content\n",
    "            return element[attr] if attr else element.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error in safe_extract for {selector}: {e}\")\n",
    "        return default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd69ff",
   "metadata": {},
   "source": [
    "### üì¶ Dataclass: `Book` \n",
    "Defines a consistent structure for storing the scraped book metadata. Using a @dataclass ensures type clarity, enforces field names, and simplifies downstream transformation into a DataFrame.\n",
    "\n",
    "The class includes fields for all expected metadata, including basic bibliographic info (title, author), content-related details (description, genres, language), statistics (ratings/reviews), and series information if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43130be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Book:\n",
    "    title: Optional[str]\n",
    "    author: Optional[str]\n",
    "    description: Optional[str]\n",
    "    genres: list[str]\n",
    "    language: Optional[str]\n",
    "    num_pages: Optional[str]\n",
    "    publication_year: Optional[str]\n",
    "    rating: Optional[str]\n",
    "    num_ratings: Optional[str]\n",
    "    num_reviews: Optional[str]\n",
    "    part_of_series: bool\n",
    "    series_name: Optional[str]\n",
    "    cover_link: Optional[str]\n",
    "    book_link: str\n",
    "    series_link: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce99f3d",
   "metadata": {},
   "source": [
    "### üìö Function: `collect_book_data`\n",
    "\n",
    "The main metadata extraction function. Given a Goodreads book URL, it fetches the HTML and parses all relevant fields.\n",
    "\n",
    "Handles:\n",
    "- Core content (title, author, genres)\n",
    "- User interaction stats (ratings, reviews)\n",
    "- Optional fields (description, series info)\n",
    "- JSON-parsed data (language metadata)\n",
    "\n",
    "The result is a complete `Book` dataclass object containing structured metadata for downstream use. Includes retry logic to handle failed requests gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a05e8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_book_data(\n",
    "    session: requests.Session,\n",
    "    book_link: str,\n",
    "    num_attempts: int = 3,\n",
    "    log_warnings: bool = False\n",
    ") -> Optional[Book]:\n",
    "    \"\"\"\n",
    "    Extracts metadata from an individual Goodreads book page.\n",
    "\n",
    "    Parameters:\n",
    "        session (requests.Session): An active requests session used to send HTTP requests.\n",
    "        book_link (str): The full URL to a Goodreads book page.\n",
    "        num_attempts (int, optional): Number of retry attempts in case of request or parsing failure. Defaults to 3.\n",
    "        logging (bool, optional): If True, logs errors during failures. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Book]: A Book dataclass instance containing extracted metadata.\n",
    "                        Returns None if all attempts fail or required fields are missing.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(num_attempts):\n",
    "        try:\n",
    "            # Send HTTP GET request to the book page\n",
    "            response = session.get(book_link, headers=user_agent)\n",
    "\n",
    "            # Check for an unsuccessful response\n",
    "            if response.status_code != 200:\n",
    "                if logging:\n",
    "                    logging.warning(f\"Failed to retrieve book at link: {book_link}. Status code: {response.status_code}\")\n",
    "\n",
    "                time.sleep(np.random.uniform(0.1, 1.0))\n",
    "                continue\n",
    "\n",
    "            # Parse page content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract core book metadata\n",
    "            title = safe_extract(soup, \"h1.Text.Text__title1\")\n",
    "            author = safe_extract(soup, \"span.ContributorLink__name\")\n",
    "            description = safe_extract(soup, \"span.Formatted\", description = True)\n",
    "\n",
    "            # Extract genres\n",
    "            genres = [genre.text for genre in soup.select(\"span.BookPageMetadataSection__genreButton\")]\n",
    "\n",
    "            # Extract language info from JSON script tag\n",
    "            language = safe_extract(soup, \"script[type='application/ld+json']\", language = True)\n",
    "\n",
    "            # Extract number of pages and publication year\n",
    "            num_pages = safe_extract(soup, \"div.FeaturedDetails\")\n",
    "            num_pages = num_pages.split()[0] if num_pages is not None else None\n",
    "\n",
    "            publication_year = safe_extract(soup, \"div.FeaturedDetails\")\n",
    "            publication_year = publication_year.split()[-1] if publication_year is not None else None\n",
    "\n",
    "            # Extract user statistics\n",
    "            rating = safe_extract(soup, \"div.RatingStatistics__rating\")\n",
    "            num_ratings = safe_extract(soup, \"span[data-testid='ratingsCount']\")\n",
    "            num_ratings = num_ratings.replace(\",\", \"\").split(\"\\xa0\")[0] if num_ratings is not None else None\n",
    "            num_reviews = safe_extract(soup, \"span[data-testid='reviewsCount']\")\n",
    "            num_reviews = num_reviews.replace(\",\", \"\").split(\"\\xa0\")[0] if num_reviews is not None else None\n",
    "\n",
    "            # Extract cover image link and series data\n",
    "            cover_link = safe_extract(soup, \"img.ResponsiveImage\", attr=\"src\")\n",
    "            series_link = safe_extract(soup, \"h3.Text.Text__title3.Text__italic.Text__regular.Text__subdued a\", attr=\"href\")\n",
    "\n",
    "            part_of_series = series_link is not None\n",
    "            series_name = None\n",
    "\n",
    "            if part_of_series:\n",
    "                series_name = safe_extract(soup, \"h3.Text.Text__title3.Text__italic.Text__regular.Text__subdued a\")\n",
    "                series_name = series_name.split(\"#\")[0].strip() if series_name is not None else None\n",
    "\n",
    "            return Book(\n",
    "                title = title,\n",
    "                author = author,\n",
    "                description = description,\n",
    "                genres = genres,\n",
    "                language = language,\n",
    "                num_pages = num_pages,\n",
    "                publication_year = publication_year,\n",
    "                rating = rating,\n",
    "                num_ratings = num_ratings,\n",
    "                num_reviews = num_reviews,\n",
    "                part_of_series = part_of_series,\n",
    "                series_name = series_name,\n",
    "                cover_link = cover_link,\n",
    "                book_link = book_link,\n",
    "                series_link = series_link\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            if logging:\n",
    "                logging.error(f\"An error occurred on book found at {book_link}: {e}\")\n",
    "            time.sleep(np.random.uniform(0.1, 1.0))\n",
    "            continue\n",
    "\n",
    "    # Return None if all attempts fail\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fb4d1",
   "metadata": {},
   "source": [
    "### üöÄ Driver Code\n",
    "\n",
    "Coordinates the end-to-end scraping process.\n",
    "\n",
    "Steps:\n",
    "1. Sets up an HTTP session with a custom User-Agent\n",
    "2. Iterates through specified Goodreads list pages\n",
    "3. Collects links to each book on the page\n",
    "4. Extracts detailed metadata from each book\n",
    "5. Aggregates results into a list of `Book` objects\n",
    "6. Converts the list into a Pandas DataFrame\n",
    "7. Saves the final dataset to a CSV file\n",
    "\n",
    "Includes logging and delay mechanisms to monitor progress and minimize risk of IP blocks from Goodreads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c2b748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 20:54:51,170 [INFO] 100 books collected\n",
      "2025-07-01 20:56:57,225 [INFO] 200 books collected\n",
      "2025-07-01 20:58:52,526 [INFO] 300 books collected\n",
      "2025-07-01 21:00:51,350 [INFO] 400 books collected\n",
      "2025-07-01 21:03:05,161 [INFO] 500 books collected\n",
      "2025-07-01 21:05:17,725 [INFO] 600 books collected\n",
      "2025-07-01 21:07:23,700 [INFO] 700 books collected\n",
      "2025-07-01 21:09:25,113 [INFO] 800 books collected\n",
      "2025-07-01 21:11:30,381 [INFO] 900 books collected\n",
      "2025-07-01 21:13:45,846 [INFO] 1000 books collected\n",
      "2025-07-01 21:15:54,724 [INFO] 1100 books collected\n",
      "2025-07-01 21:18:06,918 [INFO] 1200 books collected\n",
      "2025-07-01 21:20:18,537 [INFO] 1300 books collected\n",
      "2025-07-01 21:22:30,586 [INFO] 1400 books collected\n",
      "2025-07-01 21:25:31,827 [INFO] 1500 books collected\n",
      "2025-07-01 21:27:46,661 [INFO] 1600 books collected\n",
      "2025-07-01 21:29:50,285 [INFO] 1700 books collected\n",
      "2025-07-01 21:32:08,301 [INFO] 1800 books collected\n",
      "2025-07-01 21:34:24,503 [INFO] 1900 books collected\n",
      "2025-07-01 21:36:36,767 [INFO] 2000 books collected\n",
      "2025-07-01 21:38:54,912 [INFO] 2100 books collected\n",
      "2025-07-01 21:41:12,693 [INFO] 2200 books collected\n",
      "2025-07-01 21:43:24,903 [INFO] 2300 books collected\n",
      "2025-07-01 21:45:41,067 [INFO] 2400 books collected\n",
      "2025-07-01 21:47:52,276 [INFO] 2500 books collected\n",
      "2025-07-01 21:49:55,967 [INFO] 2600 books collected\n",
      "2025-07-01 21:52:01,706 [INFO] 2700 books collected\n",
      "2025-07-01 21:54:15,011 [INFO] 2800 books collected\n",
      "2025-07-01 21:56:24,383 [INFO] 2900 books collected\n",
      "2025-07-01 21:58:46,468 [INFO] 3000 books collected\n",
      "2025-07-01 22:01:11,767 [INFO] 3100 books collected\n",
      "2025-07-01 22:03:20,475 [INFO] 3200 books collected\n",
      "2025-07-01 22:05:42,065 [INFO] 3300 books collected\n",
      "2025-07-01 22:07:58,482 [INFO] 3400 books collected\n",
      "2025-07-01 22:10:28,008 [INFO] 3500 books collected\n",
      "2025-07-01 22:13:08,329 [INFO] 3600 books collected\n",
      "2025-07-01 22:15:22,755 [INFO] 3700 books collected\n",
      "2025-07-01 22:17:42,377 [INFO] 3800 books collected\n",
      "2025-07-01 22:19:52,542 [INFO] 3900 books collected\n",
      "2025-07-01 22:22:12,646 [INFO] 4000 books collected\n",
      "2025-07-01 22:24:25,542 [INFO] 4100 books collected\n",
      "2025-07-01 22:26:41,938 [INFO] 4200 books collected\n",
      "2025-07-01 22:28:58,834 [INFO] 4300 books collected\n",
      "2025-07-01 22:31:33,317 [INFO] 4400 books collected\n",
      "2025-07-01 22:33:47,169 [INFO] 4500 books collected\n",
      "2025-07-01 22:35:57,409 [INFO] 4600 books collected\n",
      "2025-07-01 22:38:07,771 [INFO] 4700 books collected\n",
      "2025-07-01 22:40:22,045 [INFO] 4800 books collected\n",
      "2025-07-01 22:42:44,102 [INFO] 4900 books collected\n",
      "2025-07-01 22:44:58,397 [INFO] 5000 books collected\n",
      "2025-07-01 22:47:18,914 [INFO] 5100 books collected\n",
      "2025-07-01 22:49:49,604 [INFO] 5200 books collected\n",
      "2025-07-01 22:52:10,072 [INFO] 5300 books collected\n",
      "2025-07-01 22:54:25,605 [INFO] 5400 books collected\n",
      "2025-07-01 22:56:36,163 [INFO] 5500 books collected\n",
      "2025-07-01 22:58:57,728 [INFO] 5600 books collected\n",
      "2025-07-01 23:01:22,049 [INFO] 5700 books collected\n",
      "2025-07-01 23:03:36,677 [INFO] 5800 books collected\n",
      "2025-07-01 23:05:49,561 [INFO] 5900 books collected\n",
      "2025-07-01 23:08:04,408 [INFO] 6000 books collected\n",
      "2025-07-01 23:10:31,502 [INFO] 6100 books collected\n",
      "2025-07-01 23:12:47,391 [INFO] 6200 books collected\n",
      "2025-07-01 23:16:00,599 [INFO] 6300 books collected\n",
      "2025-07-01 23:18:09,316 [INFO] 6400 books collected\n",
      "2025-07-01 23:20:22,122 [INFO] 6500 books collected\n",
      "2025-07-01 23:22:33,353 [INFO] 6600 books collected\n",
      "2025-07-01 23:24:42,475 [INFO] 6700 books collected\n",
      "2025-07-01 23:26:59,532 [INFO] 6800 books collected\n",
      "2025-07-01 23:29:31,394 [INFO] 6900 books collected\n",
      "2025-07-01 23:31:45,390 [INFO] 7000 books collected\n",
      "2025-07-01 23:33:54,544 [INFO] 7100 books collected\n",
      "2025-07-01 23:36:08,965 [INFO] 7200 books collected\n",
      "2025-07-01 23:38:14,605 [INFO] 7300 books collected\n",
      "2025-07-01 23:40:26,284 [INFO] 7400 books collected\n",
      "2025-07-01 23:42:39,203 [INFO] 7500 books collected\n",
      "2025-07-01 23:45:05,903 [INFO] 7600 books collected\n",
      "2025-07-01 23:47:18,628 [INFO] 7700 books collected\n",
      "2025-07-01 23:49:34,465 [INFO] 7800 books collected\n",
      "2025-07-01 23:51:49,152 [INFO] 7900 books collected\n",
      "2025-07-01 23:54:08,419 [INFO] 8000 books collected\n",
      "2025-07-01 23:56:25,183 [INFO] 8100 books collected\n",
      "2025-07-01 23:58:48,720 [INFO] 8200 books collected\n",
      "2025-07-02 00:01:11,174 [INFO] 8300 books collected\n",
      "2025-07-02 00:03:37,862 [INFO] 8400 books collected\n",
      "2025-07-02 00:05:54,544 [INFO] 8500 books collected\n",
      "2025-07-02 00:08:18,479 [INFO] 8600 books collected\n",
      "2025-07-02 00:10:39,025 [INFO] 8700 books collected\n",
      "2025-07-02 00:12:58,275 [INFO] 8800 books collected\n",
      "2025-07-02 00:15:14,245 [INFO] 8900 books collected\n",
      "2025-07-02 00:17:32,536 [INFO] 9000 books collected\n",
      "2025-07-02 00:19:45,234 [INFO] 9100 books collected\n",
      "2025-07-02 00:22:05,978 [INFO] 9200 books collected\n",
      "2025-07-02 00:24:21,810 [INFO] 9300 books collected\n",
      "2025-07-02 00:26:40,306 [INFO] 9400 books collected\n",
      "2025-07-02 00:28:58,327 [INFO] 9500 books collected\n",
      "2025-07-02 00:31:28,206 [INFO] 9600 books collected\n",
      "2025-07-02 00:34:00,299 [INFO] 9700 books collected\n",
      "2025-07-02 00:36:24,145 [INFO] 9800 books collected\n",
      "2025-07-02 00:38:36,036 [INFO] 9900 books collected\n",
      "2025-07-02 00:40:50,199 [INFO] Scraping complete. 9991 books collected\n",
      "2025-07-02 00:40:50,199 [INFO] Total scraping time: 227.72 minutes\n"
     ]
    }
   ],
   "source": [
    "data: list[Book] = []\n",
    "\n",
    "num_pages = 100      # Number of Goodreads list pages to scrape (After 100, the last page is repeated)\n",
    "num_attempts = 3     # Retry attempts per request\n",
    "\n",
    "start = time.time()\n",
    "session = requests.Session()\n",
    "session.headers.update(user_agent)\n",
    "\n",
    "for page_num in range(num_pages): \n",
    "    book_links = collect_book_links(session, page_num + 1, num_attempts, logging = True)\n",
    "    \n",
    "    if not book_links:\n",
    "        logging.error(f\"No book links found on page {page_num + 1}. Ending collection.\")\n",
    "        break\n",
    "        \n",
    "    for book_link in book_links:\n",
    "        book_data = collect_book_data(session, book_link, num_attempts, logging = True)\n",
    "        if book_data is None or book_data.title is None:\n",
    "            continue\n",
    "        data.append(book_data)\n",
    "        if len(data) % 100 == 0:\n",
    "            logging.info(f\"{len(data)} books collected\")\n",
    "        time.sleep(0.05)  # Small delay between requests to reduce server load\n",
    "\n",
    "# Convert the list of Book dataclass instances to DataFrame \n",
    "books_df = pd.DataFrame([book.__dict__ for book in data])\n",
    "logging.info(f\"Scraping complete. {len(books_df)} books collected\")\n",
    "\n",
    "end = time.time()\n",
    "logging.info(f\"Total scraping time: {(end - start)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c14b17c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>description</th>\n",
       "      <th>genres</th>\n",
       "      <th>language</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>part_of_series</th>\n",
       "      <th>series_name</th>\n",
       "      <th>cover_link</th>\n",
       "      <th>book_link</th>\n",
       "      <th>series_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>Winning means fame and fortune. Losing means c...</td>\n",
       "      <td>[Young Adult, Dystopia, Fiction, Fantasy, Scie...</td>\n",
       "      <td>English</td>\n",
       "      <td>374</td>\n",
       "      <td>2008</td>\n",
       "      <td>4.35</td>\n",
       "      <td>9541172</td>\n",
       "      <td>249053</td>\n",
       "      <td>True</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-th...</td>\n",
       "      <td>https://www.goodreads.com/series/73758-the-hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>Since its immediate success in 1813,  Pride an...</td>\n",
       "      <td>[Classics, Romance, Fiction, Historical Fictio...</td>\n",
       "      <td>English</td>\n",
       "      <td>279</td>\n",
       "      <td>1813</td>\n",
       "      <td>4.29</td>\n",
       "      <td>4605881</td>\n",
       "      <td>135792</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1885.Pride...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>\"Shoot all the bluejays you want, if you can h...</td>\n",
       "      <td>[Classics, Fiction, Historical Fiction, School...</td>\n",
       "      <td>English</td>\n",
       "      <td>323</td>\n",
       "      <td>1960</td>\n",
       "      <td>4.26</td>\n",
       "      <td>6658977</td>\n",
       "      <td>127709</td>\n",
       "      <td>True</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2657.To_Ki...</td>\n",
       "      <td>https://www.goodreads.com/series/255474-to-kil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>It's official: the evil Lord Voldemort has ret...</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction, Magic, Audiobo...</td>\n",
       "      <td>English</td>\n",
       "      <td>896</td>\n",
       "      <td>2003</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3674686</td>\n",
       "      <td>73546</td>\n",
       "      <td>True</td>\n",
       "      <td>Harry Potter</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>https://www.goodreads.com/book/show/58613451-h...</td>\n",
       "      <td>https://www.goodreads.com/series/45175-harry-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Book Thief</td>\n",
       "      <td>Markus Zusak</td>\n",
       "      <td>It is 1939. Nazi Germany. The country is holdi...</td>\n",
       "      <td>[Historical Fiction, Fiction, Young Adult, Cla...</td>\n",
       "      <td>English</td>\n",
       "      <td>592</td>\n",
       "      <td>2005</td>\n",
       "      <td>4.39</td>\n",
       "      <td>2790201</td>\n",
       "      <td>155737</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>https://www.goodreads.com/book/show/19063.The_...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title           author  \\\n",
       "0                           The Hunger Games  Suzanne Collins   \n",
       "1                        Pride and Prejudice      Jane Austen   \n",
       "2                      To Kill a Mockingbird       Harper Lee   \n",
       "3  Harry Potter and the Order of the Phoenix     J.K. Rowling   \n",
       "4                             The Book Thief     Markus Zusak   \n",
       "\n",
       "                                         description  \\\n",
       "0  Winning means fame and fortune. Losing means c...   \n",
       "1  Since its immediate success in 1813,  Pride an...   \n",
       "2  \"Shoot all the bluejays you want, if you can h...   \n",
       "3  It's official: the evil Lord Voldemort has ret...   \n",
       "4  It is 1939. Nazi Germany. The country is holdi...   \n",
       "\n",
       "                                              genres language num_pages  \\\n",
       "0  [Young Adult, Dystopia, Fiction, Fantasy, Scie...  English       374   \n",
       "1  [Classics, Romance, Fiction, Historical Fictio...  English       279   \n",
       "2  [Classics, Fiction, Historical Fiction, School...  English       323   \n",
       "3  [Fantasy, Young Adult, Fiction, Magic, Audiobo...  English       896   \n",
       "4  [Historical Fiction, Fiction, Young Adult, Cla...  English       592   \n",
       "\n",
       "  publication_year rating num_ratings num_reviews  part_of_series  \\\n",
       "0             2008   4.35     9541172      249053            True   \n",
       "1             1813   4.29     4605881      135792           False   \n",
       "2             1960   4.26     6658977      127709            True   \n",
       "3             2003   4.50     3674686       73546            True   \n",
       "4             2005   4.39     2790201      155737           False   \n",
       "\n",
       "             series_name                                         cover_link  \\\n",
       "0       The Hunger Games  https://images-na.ssl-images-amazon.com/images...   \n",
       "1                   None  https://images-na.ssl-images-amazon.com/images...   \n",
       "2  To Kill a Mockingbird  https://images-na.ssl-images-amazon.com/images...   \n",
       "3           Harry Potter  https://images-na.ssl-images-amazon.com/images...   \n",
       "4                   None  https://images-na.ssl-images-amazon.com/images...   \n",
       "\n",
       "                                           book_link  \\\n",
       "0  https://www.goodreads.com/book/show/2767052-th...   \n",
       "1  https://www.goodreads.com/book/show/1885.Pride...   \n",
       "2  https://www.goodreads.com/book/show/2657.To_Ki...   \n",
       "3  https://www.goodreads.com/book/show/58613451-h...   \n",
       "4  https://www.goodreads.com/book/show/19063.The_...   \n",
       "\n",
       "                                         series_link  \n",
       "0  https://www.goodreads.com/series/73758-the-hun...  \n",
       "1                                               None  \n",
       "2  https://www.goodreads.com/series/255474-to-kil...  \n",
       "3  https://www.goodreads.com/series/45175-harry-p...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f99498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset to CSV\n",
    "books_df.to_csv('../data/goodreads-books-raw.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
